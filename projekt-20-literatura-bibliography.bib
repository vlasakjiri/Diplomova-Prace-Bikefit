@book{burtbikefit,
  title     = {Bike Fit 2nd Edition: Optimise Your Bike Position for High Performance and Injury Avoidance},
  author    = {Burt, Phil},
  year      = {2022},
  publisher = {Bloomsbury Publishing}
}

@article{retulReliability,
  title        = {Does the Retül System provide reliable kinematics information for cycling analysis?},
  volume       = {11},
  url          = {https://www.jsc-journal.com/index.php/JSC/article/view/759},
  doi          = {10.28985/1322.jsc.15},
  abstractnote = {&lt;div&gt;&lt;span lang=&quot;EN-US&quot;&gt;The Retül Vantage system is a popular tool to assess dynamic positioning of cyclists. Despite of using a low sampling rate (18 Hz) to record position data, Retül measures shows a moderate to very high correlation with data from gold-standard tridimensional camera systems reaching higher sampling rates, but its reliability has not been tested. Here we assess the reliability of the Retül Vantage system for kinematic assessment of cyclists. This cross-sectional study had two phases. Phase 1 included a survey with certified Retül bike fitters to select the most common variables used in cycling kinematics assessment. Phase 2 involved assessment of the selected cycling kinematics variables to check for intra-examiner reliability. Ten bike fitters answered the online survey (response rate of 47.6%) and 7 variables were identified as the most common to conduct during bike fitting analysis. Then, ten cyclists were submitted to kinematic assessments and Vantage system variables were checked for inter-examiner reliability and standard error of the variables. Good to excellent inter-tester reliability levels were found for all the 7 kinematics variables tested. Standard error of angular variables was lower than 3º for all as well as lower than 5 mm for the linear variable tested. The minimal detectable difference values ranged from 2.15 to 6.55º for angular variables and of 15.51 mm for linear variables. A high and very high degree of intra-rater reliability can be achieved using Retül Vantage system for kinematics assessment of the most common variables included in bike fitting.&lt;/span&gt;&lt;/div&#38;gt;},
  number       = {3},
  journal      = {Journal of Science and Cycling},
  author       = {Ribeiro Branco, Guilherme and De Michelis Mendonça, Luciana and Alves Resende, Renan and Pivetta Carpes, Felipe},
  year         = {2022},
  month        = {Dec.},
  pages        = {76-84}
}

@misc{retulMarkersYoutube,
  title  = {XY Fit Steps 1-8: Markers and Harness},
  author = {{Retül Technology}},
  url    = {https://www.youtube.com/watch?v=k-FsejpibKE},
  year   = {2021}
}

@misc{retulScreenYoutube,
  title  = {XY Fit Step 4: Saddle Fore-Aft},
  author = {{Retül Technology}},
  url    = {https://www.youtube.com/watch?v=hZgI87DUUbU},
  year   = {2021}
}

@misc{bikeFastFitElitev2Youtube,
  title  = {Bike Fast Fit Elite v2 - Basic Bike Fitting},
  author = {{Bike Fast Fit}},
  url    = {https://www.youtube.com/watch?v=4xhFhvIM7R4},
  year   = {2023}
}

@misc{mmpose2020,
  title        = {OpenMMLab Pose Estimation Toolbox and Benchmark},
  author       = {MMPose Contributors},
  howpublished = {\url{https://github.com/open-mmlab/mmpose}},
  year         = {2020}
}

@inproceedings{simcc,
  title        = {Simcc: A simple coordinate classification perspective for human pose estimation},
  author       = {Li, Yanjie and Yang, Sen and Liu, Peidong and Zhang, Shoukui and Wang, Yunxiao and Wang, Zhicheng and Yang, Wankou and Xia, Shu-Tao},
  booktitle    = {European Conference on Computer Vision},
  pages        = {89--106},
  year         = {2022},
  organization = {Springer}
}

@inproceedings{vipnas,
  title     = {Vipnas: Efficient video pose estimation via neural architecture search},
  author    = {Xu, Lumin and Guan, Yingda and Jin, Sheng and Liu, Wentao and Qian, Chen and Luo, Ping and Ouyang, Wanli and Wang, Xiaogang},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {16072--16081},
  year      = {2021}
}

@inproceedings{coco,
  author    = {Lin, Tsung-Yi
               and Maire, Michael
               and Belongie, Serge
               and Hays, James
               and Perona, Pietro
               and Ramanan, Deva
               and Doll{\'a}r, Piotr
               and Zitnick, C. Lawrence},
  editor    = {Fleet, David
               and Pajdla, Tomas
               and Schiele, Bernt
               and Tuytelaars, Tinne},
  title     = {Microsoft COCO: Common Objects in Context},
  booktitle = {Computer Vision -- ECCV 2014},
  year      = {2014},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {740--755},
  abstract  = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  isbn      = {978-3-319-10602-1}
}

@inproceedings{hrnet,
  author    = {Sun, Ke and Xiao, Bin and Liu, Dong and Wang, Jingdong},
  title     = {Deep High-Resolution Representation Learning for Human Pose Estimation},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2019}
}

@article{rtmpose,
  title   = {RTMPose: Real-Time Multi-Person Pose Estimation based on MMPose},
  author  = {Jiang, Tao and Lu, Peng and Zhang, Li and Ma, Ningsheng and Han, Rui and Lyu, Chengqi and Li, Yining and Chen, Kai},
  journal = {arXiv preprint arXiv:2303.07399},
  year    = {2023}
}

@article{rtmdet,
  title   = {Rtmdet: An empirical study of designing real-time object detectors},
  author  = {Lyu, Chengqi and Zhang, Wenwei and Huang, Haian and Zhou, Yue and Wang, Yudong and Liu, Yanyi and Zhang, Shilong and Chen, Kai},
  journal = {arXiv preprint arXiv:2212.07784},
  year    = {2022}
}

@inproceedings{yoloPose,
  title     = {Yolo-pose: Enhancing yolo for multi person pose estimation using object keypoint similarity loss},
  author    = {Maji, Debapriya and Nagori, Soyeb and Mathew, Manu and Poddar, Deepak},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {2637--2646},
  year      = {2022}
}

@article{yolox,
  title   = {Yolox: Exceeding yolo series in 2021},
  author  = {Ge, Zheng and Liu, Songtao and Wang, Feng and Li, Zeming and Sun, Jian},
  journal = {arXiv preprint arXiv:2107.08430},
  year    = {2021}
}

@article{mmdetection,
  title   = {{MMDetection}: Open MMLab Detection Toolbox and Benchmark},
  author  = {Chen, Kai and Wang, Jiaqi and Pang, Jiangmiao and Cao, Yuhang and
             Xiong, Yu and Li, Xiaoxiao and Sun, Shuyang and Feng, Wansen and
             Liu, Ziwei and Xu, Jiarui and Zhang, Zheng and Cheng, Dazhi and
             Zhu, Chenchen and Cheng, Tianheng and Zhao, Qijie and Li, Buyu and
             Lu, Xin and Zhu, Rui and Wu, Yue and Dai, Jifeng and Wang, Jingdong
             and Shi, Jianping and Ouyang, Wanli and Loy, Chen Change and Lin, Dahua},
  journal = {arXiv preprint arXiv:1906.07155},
  year    = {2019}
}

@inproceedings{gau,
  title        = {Transformer quality in linear time},
  author       = {Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc},
  booktitle    = {International Conference on Machine Learning},
  pages        = {9099--9117},
  year         = {2022},
  organization = {PMLR}
}


