% This file should be replaced with your file with an thesis content.
%=========================================================================
% Authors: Michal Bidlo, Bohuslav KÅ™ena, Jaroslav Dytrych, Petr Veigend and Adam Herout 2019

% For compilation piecewise (see projekt.tex), it is necessary to uncomment it and change
% \documentclass[../projekt.tex]{subfiles}
% \begin{document}

\chapter{Introduction}
Cycling is a massively popular activity world-wide. It is enjoyed by people of all ages and skill levels, from commuters to professional athletes. To enjoy cycling, it is important to have a bike that fits the rider. Having a bike that does not fit the rider can lead to unnecessary pain or over time even injuries. Having a bike that does not fit the can also have detrimental effect on performance. Therefore, a well fitted bike is important for professional athletes and amateurs alike.

Unfortunately, the setup of one's bike is not a trivial task. The information on how to set up the bike is often comprised of rule of thumbs and general guidelines. These guidelines are often contradictory and can lead to confusion. The setup of the bike is also highly individual and depends on the rider's flexibility, riding style, goals, etc. Similarly, each bike is different and requires different setup.

This led to creation of a new profession called bikefitting. Bikefitters are experts that help cyclists with the setup of their bikes. This was traditionally done in person and relied solely on the bikefitter's experience and rudimentary tools such as plumb bobs, goniometers, etc. However, in recent years, to make it more accessible, bikefitting has been increasingly relying on technology, mainly motion capture systems. This means the bikefitters don't have to rely solely on their experience and can use data to make more informed decisions. These in-person bikefitting sessions are however often not available in all areas and can be too costly for amateur cyclists.

To make bikefitting even more accessible, there has been a rise of bikefitting applications that aim automate some of the bikefitting process. Despite the success of these applications, they are often limited to a single platform, only provide very basic information to the user or are too costly.

This work aims to create a bikefitting application that does not suffer from these limitations. The application is available in the browser, provides the user with detailed information, while being easy to use and uses lightweight SOTA pose estimation models, that run directly in the browser to keep the cost of the application low.

Chapter \ref{bikefit} explains the measurements used in bikefitting and compares several commonly used software systems for bikefitting. Chapter \ref{ch:pose_estimation} describes the pose estimation models used in this work. Chapter \ref{evaluation} evaluates the pose estimation models on videos of cyclists, filmed from the side, as this is the most common view used in bikefitting. Chapter \ref{training_pose_estimation_model_for_bikefitting} uses the best performing pose estimation model to create a new pose estimation model trained specifically for bikefitting. Chapter \ref{ch:bikefit_application} describes the bikefit application and chapter \ref{experiments} evaluates the application on real world data. Chapter \ref{conclusion} concludes the work and proposes future work.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Bikefit}
\label{bikefit}
This chapter explains the standard process of bikefitting, the motivation behind it and compares several commonly used software systems for bikefitting.

\section{An Introduction to Bikefitting}
Cycling is massively popular activity world-wide. However, having incorrectly set position on the bike can lead to unnecessary pain and injuries. Having a position that does not suit the rider can also have a drastic effect on performance.

Due to these reasons, experts, known as bikefitters help cyclists with the setup of saddle position, handlebar position and sometimes even choosing the right parts such as saddle, handlebars or cranks.

Bikefit sessions are done mostly in person and while most professional cycling teams have a bikefitting specialist that helps to set the bikes for their athletes, they are often too costly for amateur cyclists.


This section draws from Phil Burt's Bike Fit 2nd Edition: Optimise Your Bike Position for High Performance and Injury Avoidance book \cite{burtbikefit}.

\section{Bikefit Measurements}
This section describes the most common measurements used in bikefitting. These measurements can be accessed from a video using automatic bike fitting software and are simple enough to adjust by the rider themselves.

\subsection{Saddle Height}
The saddle height is a fundamental measurement that significantly impacts a rider's comfort and pedaling efficiency. It is argued to be the most important measurement in bikefitting and should be the first measurement to be adjusted \cite{burtbikefit}. Bad saddle height can even cause problems commonly associted with other measurements such as knee pain, lower back pain, neck pain, saddle sores, etc.

It is determined by considering the rider's knee angle at the bottom of the pedal stroke. Knee angle is the angle between the hip and the ankle measured at the knee joint. Burt recommends a knee angle of 35-40 degrees for average riders and even up to 30 degrees for professional cyclists \cite{burtbikefit}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\textwidth]{obrazky-figures/burt_knee_angle.png}
    \caption{Knee extension angle of 140-145 degrees, which is often referred to as 35-40 degrees (being the angle of deviation from straight leg). Is optimal for the average rider. Taken from \cite{burtbikefit}.}
    \label{fig:saddle_height}
\end{figure}



Higher saddle height can help to better recruitment of glutens and hamstrings, which can lead to more power output. However, it requires more flexibility and can lead to injury if the rider is not flexible enough. Similarly too low saddle height can increase the compressive forces on the knee and lead to pain and injury.

Proper saddle height is therefore a balance between power output and comfort. It is also important to note that the saddle height is not the only factor that affects the knee angle. The saddle fore and aft position and the cleat position also affect the knee angle.

\subsection{Saddle Setback}
Saddle setback or saddle fore and aft position refers to the horizontal position of the saddle with respect to the bottom bracket.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\textwidth]{obrazky-figures/saddle_fore_aft.jpg}
%     \caption{Saddle setback is adjusted by sliding the saddle saddle rails forward or backward in the seatpost clamp. Taken from Bike Fit Adviser's \href{https://www.youtube.com/watch?app=desktop&v=SZhWVZq2qUc}{video} on saddle setback.}
%     \label{fig:saddle_fore_aft}
% \end{figure}

Setback is most often measured at the 3 o'clock position of the crank. At this position, the rider's knee should be directly above the pedal spindle. Having the knee too far back it is harder to generate power. Having the knee too far forward can lead to knee pain due to increased forces on the kneecap \cite{burtbikefit}.

Saddle setback also affects the rider's balance on the bike. Having the saddle too far forward can lead to the rider's weight being too far forward, which can lead to hand pain because of too much weight on the handlebars. Having the saddle too far back can lead to the rider's weight being too far back, which can lead to make the front wheel feel light and make the bike harder to control.

Setback also affects the rider's hip angle. Hip angle is the angle between the shoulder and the knee measured at the hip. Having the saddle more forward can lead to a more open hip angle, which can lead to more power output and more space between the rider's torso and legs at the top of the pedal stroke. This is why time trial bikes have a more forward saddle position.



\subsection{Handlebar Height and Reach}
Handlebar height measures how high the handlebars are in relation to the saddle. Handlebar reach measures how far the handlebars are in relation to the saddle.

Handlebar height influences mainly torso angle and to and also shoulder angle. Torso angle is the angle between the shoulder and the level plane measured at the hip. It is also known as the back angle. Shoulder angle is the angle between the hip and wrist measured at the shoulder. Handlebar reach influences mainly shoulder angle.

Handlebar height can be adjusted by changing the number of spacers under the stem or by changing the stem itself. Handlebar reach can be adjusted by changing the stem length or by changing the handlebars themselves.

While optimal handlebar height and reach are highly individual, there are some general guidelines. For example, a more upright position is more comfortable and is therefore recommended for longer rides. A more aggressive position is more aerodynamic and is therefore recommended for racing. Burt recommends back angle of about 45 degrees for average riders and shoulder angle of about 90 degrees with the elbow slightly bent \cite{burtbikefit}. For faster riders, the back angle can be lowered up to 30 degrees with more open shoulder angle. For more upright riders, the back angle can be increased up to 55 degrees with more closed shoulder angle.


%%%%%%%%%%%%%%%%%%%%%%%%

\section{Existing Solutions for Automated Bikefitting}

\subsection{MyVeloFit}
\href{https://www.myvelofit.com/}{MyVeloFit}  is a web application that uses pose estimation model to predict the joint locations for a side view video of the user pedaling their bike on an indoor trainer. Based on location of these joints, joint angles are then computed. On the basis of these angles and their relation to average angles, suggestions are made for adjusting the position of the saddle and handlebars.

The fitting process starts with the rider filling out questionaire about their mobility. This is then used to adjust the recommended angle ranges. For example: if the user has lower shoulder mobility, recommended ranges for shoulder angle will be increased so the user is not stretched forward so much.

After creating the user profile, user can create a fit session for one specific bike. In the process, the user selects their fit goal (performance, comfort, or balanced) and the type of bike they are using (road, gravel, mtb, triathlon, hybrid, or stationary). This also changes the recommended angle ranges.

\subsubsection{Predicted keypoints}
MyVeloFit predicts 6 joint locations for the camera facing side of the body. Most common pose estimation models predict similar keypoints. However, keypoints commonly used to adjust the position of the saddle, such as the heel and the fifth metatarsal of the foot, are missing. The keypoints used by MyVeloFit are: ankle, knee, hip, shoulder, elbow and wrist.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{obrazky-figures/myvelofit_keypoints.png}
    \caption{Side view image with predicted keypoints in MyVeloFit.}
    \label{fig:myvelofit_keypoints}
\end{figure}

From the joint angles for every frame, some are selected for computing the joint angles at the top of the pedal stroke, front of the pedal stroke and bottom. Every position uses different angle ranges and even which angles are taken into account.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{obrazky-figures/myvelofit_top.png}
    \caption{Predicted joint angles at the top of the pedal stroke in MyVeloFit.}
    \label{fig:myvelofit_top}
\end{figure}

Based on the angles computed for parts of the pedal stroke, MyVeloFit then makes suggestions for saddle height, saddle fore and aft, handlebar height and handlebar reach.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{obrazky-figures/myvelofit_suggestions.png}
    \caption{Recommended changes to the bike position based on the angles computed by MyVeloFit.}
    \label{fig:myvelofit_suggestions}
\end{figure}

Overall, MyVeloFit is relatively easy to use and its joint predictions are fairly accurate. However, it has few disadvantages:

\begin{itemize}
    \item Only the most basic keypoints are used.
    \item Every video is converted to 30 FPS and cut down to 10 seconds.
    \item Video processing and keypoint predictions are slow (3-5 minutes).
    \item Requires subscription to get joint angles and recommended changes. Either a one time payment of 35 US dollars for access for 1 person and 1 bike for 2 weeks or 75 US dollars annually for unlimited number of bikes and people.
\end{itemize}


\subsection{RetÃ¼l}
\href{https://www.retul.com/}{RetÃ¼l} is a bike fitting system employing 3D motion capture technologys. It utilizes infrared LED markers placed on specific body points to track the rider's movements dynamically while cycling. The led markers are tracked by multiple infrared cameras placed around the rider. The cameras surprisingly capture only 18 frames per second. Despite this research \cite{retulReliability} shows that the system is relatively reliable compared to 3d motion capture systems with higher frame rates.

RetÃ¼l uses 8 markers placed on both sides of the rider's body. These markers are placed on the following locations: fifth metatarsal of the foot, heel, ankle, knee, hip, shoulder, elbow and wrist.


The markers are placed by the fitter on the rider's body. Accurate placement of the markers is crucial for the system to work properly. Even small deviations can lead to inaccurate results.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{obrazky-figures/retul_markers.png}
    \caption{Placement of the markers used by RetÃ¼l. (Screenshot from instructional instructional video by RetÃ¼l \cite{retulMarkersYoutube}.)}
    \label{fig:retul_markers}
\end{figure}

RetÃ¼l's fitting process involves setting up the bike on a trainer equipped with the system. During the session, the rider performs various motions and pedal strokes while the RetÃ¼l system captures real-time data on joint angles and movements.

Data Captured by RetÃ¼l includes a wide range of joint angles and movements such as knee angles at top of the pedal stroke and bottom of the pedal stroke, hip angles throughout the pedal stroke, shoulder, elbow, and wrist positions in relation to handlebar reach and drop, as well as ankle and foot movement concerning cleat positioning and alignment.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{obrazky-figures/retul_app.png}
    \caption{RetÃ¼l's software showing the captured data. (Screenshot from  instructional video by RetÃ¼l \cite{retulScreenYoutube}.)}
    \label{fig:retul_app}
\end{figure}

The normal ranges for these angles were constructed based on the data collected from thousands cyclists. However, these cyclists were not necessarily optimally fitted to their bikes. Therefore, the normal ranges may not be based on the optimal position for the rider.

Based on the captured data, RetÃ¼l compares the rider's position to the normal ranges. Based on this comparison, the fitter can make changes to the bike position.

Despite the fact that RetÃ¼l is a very popular bike fitting system, it has some important disadvantages:
\begin{itemize}
    \item Costly equipment and setup requirements, limiting accessibility to some individuals or smaller bike shops.
    \item The need for trained RetÃ¼l bike fitters to interpret and implement fitting recommendations effectively.
    \item Requires in-person fitting sessions. These sessions can be time-consuming and costly.
\end{itemize}


\subsection{BikeFast Fit Elite}
\href{https://www.bikefastfit.com/}{BikeFast Fit Elite} is an iOS and Mac OS application that uses pose estimation model to predict the joint locations for a side view video of the user pedaling their bike on an indoor trainer. Compared to MyVeloFit, it uses additional keypoints for the fifth metatarsal of the foot and the heel.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{obrazky-figures/bike_fast_fit_elite.png}
    \caption{Side view image with predicted keypoints in BikeFast Fit Elite. (Screenshot from promotional video \cite{bikeFastFitElitev2Youtube} by BikeFast Fit Elite.)}
    \label{fig:bikefastfit_keypoints}
\end{figure}

Similarly to MyVeloFit, it suggest changes to the saddle height and fore and aft position but it does not suggest changes to the handlebar position, arguing that the handlebar position is based on individual goals and flexibility.

Additionally, it also provides front view knee tracking to address possible knee wobble and asymmetry.

The app costs 19.99 US dollars and does not require a subscription. However, it is only available for iOS and Mac OS. Also it only captures 3.5 seconds of video.


\subsection{Kinovea}


\subsection{Posiclist}

\chapter{Marker-based Tracking}
\label{marker_based_tracking}
To estimate the keypoints, the application uses colored markers, which are placed on the parts of the body, whose position needs to be estimated (foot, heel, ankle, knee, hip, shoulder, elbow, wrist). The markers are at first localized manually by the user and their color is then used to detect them in the video. The markers are then tracked using a method similar to SORT. The detection and tracking is implemented using the OpenCV.js library \cite{opencvjs}.

\section{Localization and Tracking Pipeline}

\subsection{Manual Localization of the Markers}
\label{manual_localization_of_the_markers}
At first, the user is asked to manually localize the markers by clicking on them in the first frame of the video. Figure \ref{fig:marker_localization} shows an example of the manual localization of the markers. The colors of these markers are saved and used for thresholding and detection of the markers in the following frames. The user is asked to first localize the marker on the foot, then the marker on the heel, then the marker on the ankle, etc.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{obrazky-figures/markers_localization.png}
    \caption{Example of the manual localization of the markers. The user is asked to click on the markers in the first frame of the video. The single localized foot marker is highlighted in green.}
    \label{fig:marker_localization}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{obrazky-figures/markers_crop.png}
    \caption{Closeup of the localization of the markers. The single localized foot marker is highlighted in green. The contours of the thresholded regions are highlighted in white. The centroids of the contours are highlighted by small green cross.}
    \label{fig:marker_crop}
\end{figure}


After clicking on a marker, multiple steps are performed:
\begin{enumerate}
    \item The x,y coordinates of the marker are saved.
    \item If there is already some marker localized and therefore some color thresholds defined and markers detected, the detected markers are searched if there is some marker close to the clicked marker. If there is, the location of the clicked marker is updated to the location of the detected marker. This is done so that user does not have to precisely click on the marker. This is especially useful when the markers are small and the user is using a mobile device with a small screen.
    \item Based on the x,y coordinates of the marker, the color of the marker is extracted from the image.
    \item The color of the marker is used to update the color thresholds used to detect the markers. More on this in section \ref{detection}.
\end{enumerate}

\subsection{Detection}
\label{detection}
After obtaining the markers locations and colors, the markers are detected in the following frames. The detection is performed in the following steps:
\begin{enumerate}
    \item The frame is converted to the CIELAB color space.
    \item The frame is thresholded using the minimum and maximum A and B channel values of the localized markers and some margin. The margin is used to account for the changes in the lighting conditions. Empirically, the margin of 15 works well. Figure \ref{fig:marker_detection} shows an example of the thresholded image.
    \item Contours are found in the thresholded image. The \texttt{cv.findContours()} method is used for this.
    \item The centroids of the contours are calculated using the \texttt{cv.moments()} method.
    \item The centroids are used as noisy detections.
\end{enumerate}

The CIELAB color space is used because it is designed to approximate human vision. The A and B channels are used because they represent the color of the marker. The L channel is not used because it represents the lightness of the color and is not very useful for detecting the markers due to the changes in the lighting conditions. Experiments with using the H channel of the HSV color space were also performed, but there were some problems with the detection of the markers on the edges of the color spectrum (H=0 and H=255). The CIELAB color space does not have this problem, because it uses two channels for the color.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{obrazky-figures/thresholded.png}
    \caption{Example of the image thresholded using the minimum and maximum A and B channel values of the localized markers and a margin of 15. The markers are then detected as the contours in the thresholded image.}
    \label{fig:marker_detection}
\end{figure}

\subsection{Tracking}
\label{tracking}
The detected markers are then tracked using a simple method similar to SORT \cite{sort}. The tracking algorithm works as follows:

\begin{enumerate}
    \item Every possible assignment of the detected markers and the markers in the previous frame is created. To limit the number of possible assignments, the assignments are created only if the distance between the markers is less than 50 pixels.
    \item For every found assignment, the cost of the assignment is calculated as the sum of the distances between the markers in the assignment.
    \item To make tracking more robust, the cost of the assignment is increased by the difference of the angles between the markers in the assignment. This is done to prevent the markers from switching places.
    \item Assignment with the most markers assigned and the lowest cost is selected.
\end{enumerate}

Experiments with using the Kalman filter were also performed. Unfortunately I was not able to get the Kalman filter to work properly. The Kalman filter was not able to predict the position of the markers and the tracking was not very robust. The simple method described above works better.

\section{Limitations}
The marker-based approach was mainly chosen due to its theoretically perfect accuracy. However, in practice there are several limitations to this approach:

\begin{enumerate}
    \item Accuracy of the system heavily depends on the placement of the markers. If the markers are not placed correctly, the results are not accurate. Since the placement is done by the user, this is a real issue. Correct placement of the markers is not trivial.
    \item The user needs to have the markers available. They don't have to be special video markers, but they need to be of a different color than the background and the clothing of the user and need to be glued to the user.
    \item The biggest limitation is the reduced robustness of the system. The system works well if the color of the markers is not present elsewhere in the image. However, in other cases, the markers are detected in the wrong places and the system does not work. Figure \ref{fig:failed_localization} shows an example of the failed localization of the markers. The markers are not localized correctly, because the leaves in the background have similar color to the markers.
\end{enumerate}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{obrazky-figures/failed_localization.png}
    \caption{Example of a failed localization of the markers. The leaves in the background have similar color to the markers and the markers are therefore not localized correctly. The markers are localized as the green dots.}
    \label{fig:failed_localization}
\end{figure}

Due to these serious limitations, the marker-based approach was not used in the final version of the application. And instead, the RTMPose pose estimation model is used. The details of the implementation are described in section \ref{pose_estimation}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Pose Estimation Models}
\label{ch:pose_estimation}

This chapter describes some of the good performing pose estimation algorithms and compares their performance on the task of side view pose estimation of cyclists.

Pose estimation is a computer vision task that involves predicting the locations of keypoints on a person in an image or a video. The keypoints are usually the joints of the person, such as the ankle, knee, hip, shoulder, elbow, wrist, etc. The pose estimation models are usually trained on datasets that contain images or videos with annotated keypoints. The models are then evaluated on the same datasets.

There are multiple approaches to pose estimation. The most common approaches are the top-down approach and the bottom-up approach. The top-down approach first detects people in the image and then predicts the keypoints for each person. The bottom-up approach first predicts the keypoints and then groups them into poses using hand writte post processing algorithms. Another approach is the one-stage approach, which predicts the keypoints directly from the image without the need for post processing. This work evaluates three pose estimation models using the top-down approach (RTMPose, HRNet and ViPNAS), described in sections \ref{rtmpose}, \ref{hrnet} and \ref{vipnas} and one pose estimation model using the one-stage approach (YOLOX-Pose), described in section \ref{yolox-pose}.

Not all of the models predict the same keypoints. The models predict either 17 keypoints, as defined in the COCO dataset \cite{coco}, 26 keypoints, as defined in the Halpe26 dataset \cite{halpe}, or 133 keypoints, as defined in the COCO-WholeBody dataset \cite{coco-wholebody}.

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \centering

        \includegraphics[height=8cm]{obrazky-figures/coco_landmarks.png}
        % \caption{MS-COCO keypoints. Taken from \cite{coco}.}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.25\textwidth}
        \centering

        \includegraphics[height=8cm]{obrazky-figures/halpe_landmarks.jpg}
        % \caption{Halpe26 keypoints. Taken from \cite{halpe}.}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.5\textwidth}
        \centering

        \includegraphics[height=8cm]{obrazky-figures/coco_wholebody_landmarks.png}
        % \caption{COCO-WholeBody keypoints. Taken from \cite{coco-wholebody}.}
        \label{fig:image3}
    \end{subfigure}

    \caption{Sets of keypoints used by the MS-COCO dataset (image taken from \cite{coco}), the Halpe26 dataset (image taken from \cite{halpe}) and the COCO-WholeBody dataset (image taken from \cite{halpe}).}
    \label{fig:subfigures}
\end{figure}



\section{RTMPose}
\label{rtmpose}

RTMPose \cite{rtmpose} is a pose estimation model from the authors of the MMPose framework \cite{mmpose2020}. It is designed to bridge the gap between the excellent performance of 2D pose estimation on public benchmarks and its application in the industrial community, which still suffers from heavy models and high latency.

The RTMPose models are designed to be lightweight and fast. The authors claim, that the RTMPose-m achieves 75.8\% AP on COCO with 90+ FPS on an Intel i7-11700 CPU and 430+ FPS on an NVIDIA GTX 1660 Ti GPU. The RTMPose-s model, achieving 72.2\% AP on COCO was also tested on a mobile device with the Snapdragon 865 chip, running at 70+ FPS.

\subsection{Architecture}

The RTMPose models are based on the top-down pose estimation approach. They use a two-stage prediction, where the first stage is a person detector and the second stage is a pose estimator. The person detector is used to crop the image to the bounding box of the person. The pose estimator then predicts the keypoints for the cropped image. The authors claim that this approach is more accurate than the bottom-up approach, while still being faster in cases where the number of people in the image is lower than 6.

The CSPNeXt backbone is used in the RTMPose models. This backbone is primarly designed for object detection. Authors claim that backbones designed for image classification, are not optimal for dense tasks such as pose estimation, object detection, semantic segmentation, etc. Some backbones using high-resolution feature maps or advanced transformer architectures achieve good results, but suffer from high computional cost, high latency or difficulties in deployment. The CSPNeXt backbone is designed to have a good balance of speed and accuracy.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{obrazky-figures/rtmpose_architecture.png}
    \caption{Architecture of the RTMPose models, which comprises of a CSPNeXt backbone, a convolutional layer, a fully-connected layer, and a Gated Attention Unit (GAU) designed to enhance K keypoint representations. Subsequently, the process of 2D pose estimation is treated as two separate classification tasks for the x-axis and y-axis coordinates. This involves predicting the horizontal and vertical positions of keypoints. Taken from \cite{rtmpose}.}
    \label{fig:rtmpose_architecture}
\end{figure}

For prediction of the keypoints, the RTMPose models utilize the SimCC \cite{simcc} algorithm. SimCC reformulates human pose estimation as two classification tasks for horizontal and vertical coordinates. To reduce quantization error, SimCC can use a larger number of bins for the classification tasks. This can lead to better accuracy, while still being faster than post-processing methods commonly used with tradional heatmap based pose estimation models.

To better use spatial information, the RTMPose models use a Gated Attention Unit (GAU) \cite{gau} module. The GAU has faster speed, lower memory cost and better accuracy than the commonly used self-attention module, proposed in the Transformer architecture \cite{attentionIsAllYouNeed}.

\subsection{Model Versions}
This work evaluates the following RTMPose models:

\begin{description}
    \item[RTMPose Body8 models:] These models are trained and evaluated on the Body8 dataset consisting of 8 pose estimation datasets (AI Challenger \cite{ai_challenger}, MS-COCO \cite{coco}, CrowdPose \cite{crowdpose}, MPII \cite{mpii}, sub-JHMDB \cite{JHMDB}, Halpe \cite{halpe}, PoseTrack18 \cite{posetrack} and OCHuman \cite{ochuman}). They predict 17 keypoints, as defined in the COCO dataset.
    \item[RTMPose Halpe26 models:] These models are also trained and evaluated on the Body8 dataset. However, they predict 26 keypoints, as defined in the Halpe26 dataset.
    \item[RTMPose WholeBody models:] These models are trained and evaluated on the COCO-WholeBody \cite{coco-wholebody} and UBody \cite{ubody} datasets. They predict 133 keypoints, as defined in the COCO-WholeBody dataset.
\end{description}




\section{HRNet}
\label{hrnet}

The HRNet (High-Resolution Network) \cite{hrnet} pose estimation architecture is characterized by its emphasis on maintaining high-resolution representations throughout the network. Unlike conventional methods that downsample the input image early in the network, HRNet adopts a multi-resolution approach, preserving detailed information essential for accurate pose estimation.

A notable feature of the HRNet pose estimation architecture is its ability to capture both local and global context effectively. The network consists of parallel branches, each processing different resolutions of the input image, and these branches are interconnected, facilitating the exchange of information between different scales. This design allows HRNet to address challenges associated with scale variations in human poses, ensuring robust performance in capturing both fine details and overall pose structure.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{obrazky-figures/hrnet_architecture.png}
    \caption{Architecture of the HRNet models. The HRNet models consists of three parallel branches, each processing different resolutions of the input image. These branches are interconnected, facilitating the exchange of information between different scales. Taken from \cite{hrnet}.}
    \label{fig:hrnet_architecture}
\end{figure}

This work evaluates the HRNet-W32 model, which uses heatmap to predict the keypoints. The HRNet-W32 model is trained on the COCO dataset and predicts 17 keypoints, as defined in the COCO dataset.


\section{YOLOX-Pose}
\label{yolox-pose}

YOLOX-Pose  is a one-stage human pose estimation model that distinguishes itself from traditional top-down and bottom-up methods. The authors claim that YOLOX-Pose is the first one-stage human pose estimation model that achieves comparable performance to the state-of-the-art two-stage methods.

The one-stage approach has some advantages over bottom-up and top-down approaches. Bottom-up approaches first detect keypoints and then group them into poses. The grouping step is not learned and needs to be hand-crafted. This can lead to inaccurate results, especially in cases where the keypoints from different people are close together.

Top-down approaches first detect people and then predict keypoints for each person. This approach is favored by SOTA models, but the processing time grows linearly with the number of people in the image. This can lead to slow processing times in cases where there are many people in the image. SOTA methods also often use heavy detectors, which further increases the processing time.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{obrazky-figures/yolopose_architecture.png}
    \caption{Architecture of the original YOLO-Pose model, which was based on the anchor-based YOLOv5 detector. The YOLOX-Pose model, which is evaluated in this work, is based on the anchorless YOLOX detector. Taken from \cite{yoloPose}.}
    \label{fig:yolopose_architecture}
\end{figure}

The original YOLO-pose \cite{yoloPose} model uses the anchor-based YOLOv5 detector \cite{yolov5}. The YOLOX-Pose model, which is evaluated in this work, uses the YOLOX detector \cite{yolox}. The YOLOX detector is based on the YOLOv5 detector, but uses anchor-free approach, which does not require the anchor boxes to be predefined. This allows the YOLOX detector to be more flexible. The YOLOX detector generally outperforms the YOLOv5 detector on the COCO dataset.

The YOLOX-Pose model is trained on the COCO dataset and predicts 17 keypoints, as defined in the COCO dataset.

\section{ViPNAS}
\label{vipnas}

The ViPNAS (Video Pose Estimation via Neural Architecture Search) \cite{vipnas} model focuses on video pose estimation, employing two networks: S-ViPNet and T-ViPNet.

The S-ViPNet is used on invidual frames and keyframes in the video to predict the keypoints. The lightweight T-ViPNet network is used on non-keyframes to propagate the pose estimations. It extracts features from the current frame and also fuses heatmaps from the previous frame. The fused features are then used to predict the keypoints.

To find the optimal architectures for the S-ViPNet and T-ViPNet networks, the authors used neural architecture search (NAS). The search space for the S-ViPNet network is called the spatial-level search space. It consists of 5 dimensions (depth, width, kernel size, group and attention). The search space for the T-ViPNet network is called the temporal-level search space. It searches for the optimal fusion operation (addition, multiplication or concatenation) and the best stage of features to fuse.

This work evaluates the modified S-ViPNet model with the MobileNetV3 \cite{mobilenetv3} backbone and SimCC \cite{simcc} algorithm instead of predicting heatmaps. The model is trained on the COCO dataset and predicts 17 keypoints, as defined in the COCO dataset.

\section{Results On The COCO Keypoint Dataset}
To compare the potential of the methods, the results of the models on the COCO dataset are shown in table \ref{tab:evaluation_results_coco}. All of the methods were trained only on the COCO dataset and predict the same keypoints. The results are taken from MMPose documentation \cite{mmpose2020}.

The RTMPose models achieve strong results, with the RTMPose-l-256x192 model achieving the best results. The HRNet-W32 model also performs well but it is a much larger and slower model. However, the other lightweight models such as YOLOX-Pose and ViPNAS-MobileNetV3 perform significantly worse than even the medium and small RTMPose models. Overall the RTMPose models show very promising results compared to the other models.

\begin{table}[htbp]
    \setlength{\tabcolsep}{4pt}
    \centering
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Model}              & \textbf{Input Size} & \textbf{AP}    & \textbf{AP-50} & \textbf{AP-75} & \textbf{AR}    & \textbf{AR-50} \\
        \midrule
        \textbf{rtmpose-l}          & \textbf{256x192}    & \textbf{0.758} & \textbf{0.906} & \textbf{0.826} & \textbf{0.806} & 0.942          \\
        pose\_hrnet\_w32            & 256x192             & 0.749          & 0.906          & 0.821          & 0.804          & \textbf{0.945} \\
        rtmpose-m                   & 256x192             & 0.746          & 0.899          & 0.817          & 0.795          & 0.935          \\
        rtmpose-s                   & 256x192             & 0.716          & 0.892          & 0.789          & 0.768          & 0.929          \\
        simcc\_S-ViPNAS-MobileNetV3 & 256x192             & 0.695          & 0.883          & 0.772          & 0.755          & 0.927          \\
        yoloxpose\_m                & 640x640             & 0.695          & 0.899          & 0.766          & 0.733          & 0.926          \\
        rtmpose-t                   & 256x192             & 0.682          & 0.883          & 0.759          & 0.736          & 0.920          \\


        \bottomrule
    \end{tabular}
    \caption{Results of the models on the COCO dataset. The results are sorted by the Average precision (AP). The AP describes the average precision over Object Keypoint Similarity (OKS) thresholds from 0.5 to 0.95 with a step size of 0.05. The AR describes the average recall over OKS thresholds from 0.5 to 0.95 with a step size of 0.05. The AP-50 and AR-50 are the same as AP and AR, but only for OKS threshold of 0.5. Similarly AR-75 is average recall with OKS threshold of 0.75. The results are taken from MMPose documentation \cite{mmpose2020}.}
    \label{tab:evaluation_results_coco}

\end{table}

\chapter{Pose Estimation Dataset for Bikefitting}



\section{Obtaining Ground Truth Annotations}
\section{Training and Validation Split}
\label{evaluation_dataset}
To accurately measure the performance of a pose estimation model, it is necessary to have a dataset with ground truth annotations. Since there is no such dataset for side view pose estimation of cyclists, new small evaluation dataset was created. It consists of 12 videos in 3 different enviroments with various camera angles and riding positions (riding on hoods, drops, tops and aero position on the drops). The videos were shot in FullHD resolution at 30 FPS.

Before shooting the videos, orange colored markers were placed on the camera-facing side of the rider's body. These markers were placed on the following locations: fifth metatarsal of the foot, heel, ankle, knee, hip, shoulder, elbow and wrist.

\begin{figure}[htbp]

    \centering
    \includegraphics[width=1\linewidth]{obrazky-figures/eval_dataset_in_hoods.png}

    \includegraphics[width=1\linewidth]{obrazky-figures/eval_dataset_out_aero.png}

    \caption{Examples of the evaluation dataset. The first video shows the rider pedaling indoors on the hoods. The second video shows the rider pedaling outdoors in the aero position on the hoods. The orange stickers are used as ground truth annotations.}


    \label{fig:evaluation_markers}
\end{figure}

To annotate the videos, the markers were tracked using the first version of the bikefit application described in section \ref{marker_based_tracking}. The application tracks the markers in a lower resolution. To get the ground truth annotations, the marker positions were then scaled to the original resolution of the video.













%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Fine-Tuning and Evaluation of Pose Estimation Models for Bikefitting}
\label{training_pose_estimation_model_for_bikefitting}


The goal of this section is to evaluate the performance of pose estimation models on the task of side view pose estimation of cyclists and find the best model for use in the bikefit application.

The models are evaluated on a custom dataset, which is described in section \ref{evaluation_dataset}. The evaluation metrics are described in section \ref{evaluation_metrics}. The results are shown in section \ref{evaluation_results}.

The models are ran using the MMPose framework \cite{mmpose2020}, with the Inferencer API and the PyTorch backend. Topdown models (models that require inputs cropped to the bounding box of the person) use the RTMDet-nano model \cite{rtmdet} from the MMDetetection framework \cite{mmdetection} for person detection.

Since no models predict the fifth metatarsal of the foot, the small toe landmark is used instead for the models that predict it.

\section{Evaluation Metrics}
\label{evaluation_metrics}

In assessing the performance of pose estimation models, widely used metrics, such as Mean Average Precision (mAP), or Object Keypoint Similarity (OKS) may not adequately gauge accuracy in scenarios involving video data where all keypoints of interest are consistently visible and accurately predicted. Similarly, relying solely on the L2 distance for evaluation proves inadequate due to the inherent variations in scale among different videos.

As an alternative approach, a novel metric called \textbf{Bounding Box Normalized Distance (BBND)} has been employed. This metric involves calculating the ratio of the Euclidean distance to the average dimension of the bounding box, scaled by a factor of 100, so it can be expressed as a percentage:

$$ \text{BBND} = \frac{\text{dist}}{\frac{\text{bbox.w} + \text{bbox.h}}{2}} \times 100 $$

Here, in the BBND formula:
\begin{description}
    \item[$\text{BBND}$:] Bounding Box Normalized Distance, which is the evaluation metric being proposed.
    \item[$\text{dist}$:] Euclidean distance between the predicted keypoints and the ground truth keypoints.
    \item[$\text{bbox.w}$:] Width of the bounding box encompassing the keypoints.
    \item[$\text{bbox.h}$:] Height of the bounding box encompassing the keypoints.
\end{description}

This adjustment allows for a scaled representation of the distance metric, effectively normalizing it with respect to the average bounding box dimension. BBND offers easy to interpret results, with a lower value indicating a more accurate prediction. It also allows for a more direct comparison of the performance of different models, as the metric is not affected by the scale of the video.

\section{Initial Evaluation of Pre-Trained Models}
\label{evaluation_results}
At first the models are evaluated using only the keypoints that all models predict. These keypoints are the ankle, knee, hip, shoulder, elbow and wrist. The results are shown in table \ref{tab:evaluation_results_all}. The best results are achieved by the rtmpose-l-256x192 model, but most of the bigger RTMPose models outperform the other models. This is in contrast to the results on the COCO dataset, where HRNet-W32 model performed similarly to the RTMPose-l model. The decrease in performance of the HRNet-W32 model is probably caused by the fact that it was trained only on the COCO dataset, while the RTMPose models were trained on multiple datasets and are therefore better used to pose estimation for cycling.

The best model achieves a mean BBND of 2.59\%. This means that the average distance between the predicted keypoints and the ground truth keypoints is 2.59\% of the average bounding box dimension.

The good results of the RTMPose models are not surprising, since they were trained on multiple datasets, while the other models were trained only on the COCO dataset. The RTMPose models trained to predict the COCO keypoints perform slightly better than the halpe26 models, which were trained to predict 26 keypoints and significantly better than the wholebody models, which were trained to predict 133 keypoints.

\begin{table}[htbp]
    \setlength{\tabcolsep}{4pt}

    \begin{tabular}{l ccccccc}
        \toprule
        \textbf{Model}                   & \textbf{Ankle} & \textbf{Knee} & \textbf{Hip}  & \textbf{Shoulder} & \textbf{Elbow} & \textbf{Wrist} & \textbf{Mean} \\

        \midrule
        rtmpose-l\_body8-256x192         & 3.43           & 3.79          & \textbf{2.15} & 3.39              & \textbf{1.21}  & 1.58           & \textbf{2.59} \\
        rtmpose-l\_body8-384x288         & 3.26           & \textbf{3.53} & 3.33          & \textbf{2.93}     & 1.44           & \textbf{1.43}  & 2.65          \\
        rtmpose-l-halpe26-256x192        & 3.31           & 3.75          & 2.74          & 3.42              & 1.27           & 1.85           & 2.72          \\
        rtmpose-m\_body8-384x288         & 3.40           & 3.84          & 3.27          & 3.26              & 1.34           & 1.59           & 2.78          \\
        rtmpose-m-halpe26-384x288        & 3.76           & 3.68          & 3.96          & 3.02              & 1.34           & 1.65           & 2.90          \\
        rtmpose-m-halpe26-256x192        & 3.56           & 3.92          & 2.97          & 3.34              & 1.71           & 2.11           & 2.94          \\
        rtmpose-m\_body8-256x192         & 4.10           & 4.19          & 2.84          & 3.50              & 1.58           & 2.08           & 3.05          \\
        rtmpose-s\_body8-256x192         & 5.19           & 4.40          & 2.83          & 3.93              & 1.95           & 2.20           & 3.42          \\
        rtmpose-s-halpe26-256x192        & 4.74           & 4.38          & 2.89          & 3.94              & 2.30           & 2.47           & 3.45          \\
        td-hm\_hrnet-w32\_coco-256x192   & \textbf{3.17}  & 4.46          & 4.31          & 4.17              & 2.37           & 2.30           & 3.46          \\
        yoloxpose\_m\_coco-640           & 3.77           & 5.68          & 3.66          & 4.20              & 2.06           & 2.55           & 3.65          \\
        rtmpose-t-halpe26-256x192        & 5.09           & 4.96          & 3.19          & 4.86              & 2.95           & 2.65           & 3.95          \\
        rtmpose-l-wholebody-256x192      & 3.65           & 5.16          & 5.01          & 4.52              & 3.41           & 2.81           & 4.09          \\
        rtmpose-t\_body8-256x192         & 6.17           & 4.98          & 3.16          & 4.70              & 2.99           & 2.75           & 4.13          \\
        simcc\_vipnas-mbv3\_coco-256x192 & 6.39           & 6.00          & 4.53          & 4.36              & 3.01           & 2.86           & 4.53          \\
        rtmpose-m-wholebody-256x192      & 5.57           & 6.16          & 4.94          & 4.78              & 3.62           & 3.44           & 4.75          \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation results on the custom bike fitting dataset, using only MS-COCO keypoints (excluding heel and fifth metatarsal of the foot). Each column represents the mean Bounding Box Normalized Distance for the given keypoint. The results are sorted by the mean Bounding Box Normalized Distance, defined in section \ref{evaluation_metrics}. The best results are highlighted in bold. The model names are derived from their MMPose config files.}
    \label{tab:evaluation_results_all}

\end{table}

The models which also output heel and top of the foot keypoints are evaluated in table \ref{tab:evaluation_results_wholebody}. These models have a slighty higher mean BBND than the models that only output the MS-COCO keypoints. This is probably because there are more datasets with MS-COCO keypoints than datasets with the heel and foot keypoints. The best model is rtmpose-l\_halpe26-256x192 with a mean BBND of 3.33\%. However, we can see that the heel and especially the fifth metatarsal of the foot are not predicted very well. This is probably because the heel and the fifth metatarsal of the foot are not visible in most of the training datasets. Another reason could be that the models do not output position of the fifth metatarsal of the foot, but only the position of the small toe and these two keypoints are not always in the same position.

\begin{table}[htbp]
    % \setlength{\tabcolsep}{4pt}
    \centering
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model}               & \textbf{Foot} & \textbf{Heel} & \textbf{Overall Mean} \\

        \midrule
        rtmpose-l\_halpe26-256x192   & \textbf{5.96} & 4.34          & \textbf{3.33}         \\
        rtmpose-m\_halpe26-384x288   & 6.36          & \textbf{4.22} & 3.50                  \\
        rtmpose-m\_halpe26-256x192   & 5.97          & 5.34          & 3.62                  \\
        rtmpose-s\_halpe26-256x192   & 6.01          & 6.23          & 4.12                  \\
        rtmpose-t\_halpe26-256x192   & 6.78          & 6.45          & 4.62                  \\
        rtmpose-l\_wholebody-256x192 & 6.49          & 6.23          & 4.66                  \\
        rtmpose-m\_wholebody-256x192 & 8.13          & 8.12          & 5.59                  \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation results on the custom bike fitting dataset, including the heel and fifth metatarsal of the foot. Results for the other keypoints can be found in table \ref{tab:evaluation_results_coco}. Each column represents the mean Bounding Box Normalized Distance for the given keypoint. The results are sorted by the overall mean Bounding Box Normalized Distance (spanning all keypoints), defined in section \ref{evaluation_metrics}. The best results are highlighted in bold. The model names are derived from their MMPose config files.}
    \label{tab:evaluation_results_wholebody}

\end{table}


\section{Fine-Tuning On Custom Bikefit Dataset}

\section{Evaluation Of Fine-Tuned Models}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Implementation of the Bikefit Web Application}
\label{ch:bikefit_application}

This chapter describes the web application for bike fitting. Implementing the application as a web application has several advantages over implementing it as a mobile or desktop application. The web application can be used on any device with a web browser. This allows the application to be used on mobile devices, tablets, laptops and desktop computers. The user also does not need to download and install anything.

Most web applications are implemented using the client-server architecture with the video processing and other more complicated tasks (such as pose estimation) being performed on the server. This has several disadvantages. The server needs to be powerful enough to handle the load of multiple users. This can be quite expensive for heavy models and a lot of users. The user data also needs to be sent to the server, which means the user needs to be online when using the application and the users can be concerned about their privacy.

To avoid these disadvantages, most of the bussiness logic of the application (including video processing, keypoint estimation and generation of the suggestions) is implemented on the client side.

The application is implemented using the Svelte framework\footnote{\url{https://svelte.dev/}} . Svelte is a component framework similar to React\footnote{\url{https://react.dev/}} or Vue\footnote{\url{https://vuejs.org/}}. It is used to create reactive user interfaces. Svelte compiles the application to vanilla JavaScript, which means the application does not need to include a large framework library. This makes the application smaller and faster. Svelte also has a built-in state management system, which is used to store the application state.

To process the video frame by frame, the application uses the WebCodecs API. The details of the implementation are described in section \ref{video_upload_and_processing}.

To estimate the keypoints, multiple approaches were tested. First approach uses colored markers, which are placed on the parts of the body, whose position needs to be estimated (foot, heel, ankle, knee, hip, shoulder, elbow, wrist). The markers are then detected using color thresholding and tracked using a method similar to SORT. This approach is implemented using the OpenCV.js library \cite{opencvjs}. The details of the implementation are described in section \ref{marker_based_tracking}.

The second approach uses the RTMPose pose estimation model and a person detector. The models run in the browser using the Tensorflow.js library\footnote{\url{https://www.tensorflow.org/js}}. The models are described in section \ref{rtmpose} and evaluated in chapter \ref{evaluation}. To improve the performance of the pose estimation models on the task of side view pose estimation of cyclists, the models are trained on a custom dataset. The details of the training are described in section \ref{training_pose_estimation_model_for_bikefitting}. The details of the implementation are described in section \ref{pose_estimation}.


\section{Video Upload and Processing}
\label{video_upload_and_processing}
At first, the video needs to be uploaded to the application. The video is uploaded using the HTML5 file input. The file is then converted to a URL using the \texttt{URL.createObjectURL()} method, because the WebCodecs API requires a URL as input.

\subsection{Accessing the Video Frames}

To accurately estimate the keypoints, the application needs to access all of the video frames and process them. This is not trivial to do in the browser. The browser does not allow the application to access the video frames directly.

To process the video frame by frame, the application uses the WebCodecs API\footnote{\url{https://developer.mozilla.org/en-US/docs/Web/API/WebCodecs_API}}. The WebCodecs API is a low-level API for encoding and decoding audio and video. It gives developers access to the invididual frames of the video. It is the only way to access the frames of the video in the browser. Simple and commonly used method is to simultaneously play the video and draw the video  frames to a canvas element and then access the pixels of the canvas. However, this method only does not work well if the processing is computationally expensive, since the video is played in real time. This leads to skipping of the frames and not all of the frames are processed. The WebCodecs API allows the application to process the video at its own pace.

Because the API is relatively new, it is not yet supported by all browsers. Figure \ref{fig:caniuseWebcodecs} shows the browser support for the WebCodecs API as of January 2024. The API is supported by all browsers based on Chromium (Google Chrome, Microsoft Edge, Opera, Brave, etc.). It is Partially supported by Safari and Safari on iOS (video only) and not supported by Firefox. Overall based on CanIUse data\footnote{\url{https://caniuse.com/webcodecs}}, the API is supported by 87.6\% of the users, where 74.5\% of the users have full support and 13.1\% of the users have partial support.


\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{obrazky-figures/caniuseWebcodecs.png}
    \caption{Browser support as of January 2024 for the WebCodecs API. Green color indicates full support, yellow color indicates partial support and red color indicates no support. Taken from \url{https://caniuse.com/webcodecs}.}
    \label{fig:caniuseWebcodecs}
\end{figure}

Due to the low-level design of the WebCodecs API, it is not easy to use. Moreover, the API is quite new and not very well documented so there is not a lot of examples. The developer also needs to take care of demuxing the media containers. This work uses the getVideoFrames.js library\footnote{\url{https://github.com/josephrocca/getVideoFrames.js}} from josephrocca. The library internally uses the WebCodecs API and MP4Box.js\footnote{\url{https://github.com/gpac/mp4box.js/}} for demuxing mp4 files, but provides a simple interface for accessing the video frames. The library is used to get the video frames and then the frames are processed as described in sections \ref{marker_based_tracking} and \ref{pose_estimation}. Listing \ref{getVideoFrames} shows an example usage of the getVideoFrames.js library. It defines three callback functions. The \texttt{onFrame} callback is called for each frame of the video. The \texttt{onConfig} callback is called when the video metadata is loaded. The \texttt{onFinish} callback is called when all of the frames are processed.

\begin{lstlisting}[style=htmlcssjs, caption={Example usage of the getVideoFrames.js library. Taken from \url{https://github.com/josephrocca/getVideoFrames.js/}.}, label={getVideoFrames}]
    <canvas id="canvasEl"></canvas>
    <br>
    <input type="file" accept="video/mp4" onchange="start(this.files[0])">
    <script type="module">
      import getVideoFrames from "https://deno.land/x/get_video_frames@v0.0.10/mod.js"
      
      let frameCount = 0;
    
      window.start = async function(file) {
        let ctx = canvasEl.getContext("2d"); 
    
        // `getVideoFrames` requires a video URL as input.
        // If you have a file/blob instead of a videoUrl, turn it into a URL like this:
        let videoUrl = URL.createObjectURL(file);
    
        await getVideoFrames({
          videoUrl,
          onFrame(frame) {  // `frame` is a VideoFrame object: https://developer.mozilla.org/en-US/docs/Web/API/VideoFrame
            ctx.drawImage(frame, 0, 0, canvasEl.width, canvasEl.height);
            frame.close();
            frameCount++;
          },
          onConfig(config) {
            canvasEl.width = config.codedWidth;
            canvasEl.height = config.codedHeight;
          },
          onFinish() {
            console.log("finished!");
            console.log("frameCount", frameCount);
          },
        });
    
        URL.revokeObjectURL(file); // revoke URL to prevent memory leak
      }
    </script> 
\end{lstlisting}



\section{Pose Estimation}
\label{pose_estimation}

\subsection{Model Conversion and Deployment}

\subsection{Preprocessing and Postprocessing}



\section{Video Player}
\label{video_player}

\section{Presentation of the Results and Recommendations}

\section{User Authentication and Data Storage}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Experiments}
\label{experiments}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Conclusion}
\label{conclusion}



%=========================================================================

% For compilation piecewise (see projekt.tex), it is necessary to uncomment it
% \end{document}